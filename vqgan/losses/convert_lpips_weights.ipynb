{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Learned Perceptual Image Patch Similarity\n",
    "\n",
    "Convert the weights from the official repo for Learned Perceptual Image Patch Similarity.\n",
    "\n",
    "Code from: [https://github.com/richzhang/PerceptualSimilarity/blob/master/lpips/lpips.py]()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aB9FYTsAd1sZ"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import vgg16\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lpips import LPIPS as LPIPSTF\n",
    "\n",
    "URL_MAP = {\n",
    "    \"vgg_lpips\": \"https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1\"\n",
    "}\n",
    "\n",
    "CKPT_MAP = {\n",
    "    \"vgg_lpips\": \"vgg.pth\"\n",
    "}\n",
    "\n",
    "MD5_MAP = {\n",
    "    \"vgg_lpips\": \"d507d7349b931f0638a25a48a722f98a\"\n",
    "}\n",
    "\n",
    "\n",
    "def download(url, local_path, chunk_size=1024):\n",
    "    os.makedirs(os.path.split(local_path)[0], exist_ok=True)\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        total_size = int(r.headers.get(\"content-length\", 0))\n",
    "        with tqdm(total=total_size, unit=\"B\", unit_scale=True) as pbar:\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                for data in r.iter_content(chunk_size=chunk_size):\n",
    "                    if data:\n",
    "                        f.write(data)\n",
    "                        pbar.update(chunk_size)\n",
    "\n",
    "\n",
    "def md5_hash(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        content = f.read()\n",
    "    return hashlib.md5(content).hexdigest()\n",
    "\n",
    "\n",
    "def get_ckpt_path(name, root, check=False):\n",
    "    assert name in URL_MAP\n",
    "    path = os.path.join(root, CKPT_MAP[name])\n",
    "    if not os.path.exists(path) or (check and not md5_hash(path) == MD5_MAP[name]):\n",
    "        print(\"Downloading {} model from {} to {}\".format(name, URL_MAP[name], path))\n",
    "        download(URL_MAP[name], path)\n",
    "        md5 = md5_hash(path)\n",
    "        assert md5 == MD5_MAP[name], md5\n",
    "    return path\n",
    "\n",
    "\n",
    "class KeyNotFoundError(Exception):\n",
    "    def __init__(self, cause, keys=None, visited=None):\n",
    "        self.cause = cause\n",
    "        self.keys = keys\n",
    "        self.visited = visited\n",
    "        messages = list()\n",
    "        if keys is not None:\n",
    "            messages.append(\"Key not found: {}\".format(keys))\n",
    "        if visited is not None:\n",
    "            messages.append(\"Visited: {}\".format(visited))\n",
    "        messages.append(\"Cause:\\n{}\".format(cause))\n",
    "        message = \"\\n\".join(messages)\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "def retrieve(\n",
    "        list_or_dict, key, splitval=\"/\", default=None, expand=True, pass_success=False\n",
    "):\n",
    "    \"\"\"Given a nested list or dict return the desired value at key expanding\n",
    "    callable nodes if necessary and :attr:`expand` is ``True``. The expansion\n",
    "    is done in-place.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        list_or_dict : list or dict\n",
    "            Possibly nested list or dictionary.\n",
    "        key : str\n",
    "            key/to/value, path like string describing all keys necessary to\n",
    "            consider to get to the desired value. List indices can also be\n",
    "            passed here.\n",
    "        splitval : str\n",
    "            String that defines the delimiter between keys of the\n",
    "            different depth levels in `key`.\n",
    "        default : obj\n",
    "            Value returned if :attr:`key` is not found.\n",
    "        expand : bool\n",
    "            Whether to expand callable nodes on the path or not.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        The desired value or if :attr:`default` is not ``None`` and the\n",
    "        :attr:`key` is not found returns ``default``.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "        Exception if ``key`` not in ``list_or_dict`` and :attr:`default` is\n",
    "        ``None``.\n",
    "    \"\"\"\n",
    "\n",
    "    keys = key.split(splitval)\n",
    "\n",
    "    success = True\n",
    "    try:\n",
    "        visited = []\n",
    "        parent = None\n",
    "        last_key = None\n",
    "        for key in keys:\n",
    "            if callable(list_or_dict):\n",
    "                if not expand:\n",
    "                    raise KeyNotFoundError(\n",
    "                        ValueError(\n",
    "                            \"Trying to get past callable node with expand=False.\"\n",
    "                        ),\n",
    "                        keys=keys,\n",
    "                        visited=visited,\n",
    "                    )\n",
    "                list_or_dict = list_or_dict()\n",
    "                parent[last_key] = list_or_dict\n",
    "\n",
    "            last_key = key\n",
    "            parent = list_or_dict\n",
    "\n",
    "            try:\n",
    "                if isinstance(list_or_dict, dict):\n",
    "                    list_or_dict = list_or_dict[key]\n",
    "                else:\n",
    "                    list_or_dict = list_or_dict[int(key)]\n",
    "            except (KeyError, IndexError, ValueError) as e:\n",
    "                raise KeyNotFoundError(e, keys=keys, visited=visited)\n",
    "\n",
    "            visited += [key]\n",
    "        # final expansion of retrieved value\n",
    "        if expand and callable(list_or_dict):\n",
    "            list_or_dict = list_or_dict()\n",
    "            parent[last_key] = list_or_dict\n",
    "    except KeyNotFoundError as e:\n",
    "        if default is None:\n",
    "            raise e\n",
    "        else:\n",
    "            list_or_dict = default\n",
    "            success = False\n",
    "\n",
    "    if not pass_success:\n",
    "        return list_or_dict\n",
    "    else:\n",
    "        return list_or_dict, success\n",
    "\n",
    "\n",
    "class LPIPS(nn.Module):\n",
    "    # Learned perceptual metric\n",
    "    def __init__(self, use_dropout=True):\n",
    "        super().__init__()\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "        self.chns = [64, 128, 256, 512, 512]  # vg16 features\n",
    "        self.net = vgg16(pretrained=True, requires_grad=False)\n",
    "        self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n",
    "        self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n",
    "        self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n",
    "        self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n",
    "        self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n",
    "        self.load_from_pretrained()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def load_from_pretrained(self, name=\"vgg_lpips\"):\n",
    "        ckpt = get_ckpt_path(name, \"taming/modules/autoencoder/lpips\")\n",
    "        self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n",
    "        print(\"loaded pretrained LPIPS loss from {}\".format(ckpt))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, name=\"vgg_lpips\"):\n",
    "        if name != \"vgg_lpips\":\n",
    "            raise NotImplementedError\n",
    "        model = cls()\n",
    "        ckpt = get_ckpt_path(name)\n",
    "        model.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n",
    "        return model\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        in0_input, in1_input = (self.scaling_layer(input), self.scaling_layer(target))\n",
    "        outs0, outs1 = self.net(in0_input), self.net(in1_input)\n",
    "        feats0, feats1, diffs = {}, {}, {}\n",
    "        lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
    "        for kk in range(len(self.chns)):\n",
    "            feats0[kk], feats1[kk] = normalize_tensor(outs0[kk]), normalize_tensor(outs1[kk])\n",
    "            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n",
    "\n",
    "        res = [spatial_average(lins[kk].model(diffs[kk]), keepdim=True) for kk in range(len(self.chns))]\n",
    "        val = res[0]\n",
    "        for l in range(1, len(self.chns)):\n",
    "            val += res[l]\n",
    "        return val\n",
    "\n",
    "\n",
    "class ScalingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScalingLayer, self).__init__()\n",
    "        self.register_buffer('shift', torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n",
    "        self.register_buffer('scale', torch.Tensor([.458, .448, .450])[None, :, None, None])\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return (inp - self.shift) / self.scale\n",
    "\n",
    "\n",
    "class NetLinLayer(nn.Module):\n",
    "    \"\"\" A single linear layer which does a 1x1 conv \"\"\"\n",
    "\n",
    "    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n",
    "        super(NetLinLayer, self).__init__()\n",
    "        layers = [nn.Dropout(), ] if (use_dropout) else []\n",
    "        layers += [nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False), ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class vgg16(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True):\n",
    "        super(vgg16, self).__init__()\n",
    "        vgg_pretrained_features = models.vgg16(pretrained=pretrained).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(23, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5_3 = h\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n",
    "        return out\n",
    "\n",
    "\n",
    "def normalize_tensor(x, eps=1e-10):\n",
    "    norm_factor = torch.sqrt(torch.sum(x ** 2, dim=1, keepdim=True))\n",
    "    return x / (norm_factor + eps)\n",
    "\n",
    "\n",
    "def spatial_average(x, keepdim=True):\n",
    "    return x.mean([2, 3], keepdim=keepdim)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pytorch_model = LPIPS().to(device)\n",
    "pytorch_model.eval()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3IhU35peRp_",
    "outputId": "31bf2f5e-23ca-44ce-ed40-6cec719495ed"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LPIPS(\n",
       "  (scaling_layer): ScalingLayer()\n",
       "  (net): vgg16(\n",
       "    (slice1): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (slice2): Sequential(\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (slice3): Sequential(\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "    )\n",
       "    (slice4): Sequential(\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "    )\n",
       "    (slice5): Sequential(\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (lin0): NetLinLayer(\n",
       "    (model): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (lin1): NetLinLayer(\n",
       "    (model): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (lin2): NetLinLayer(\n",
       "    (model): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (lin3): NetLinLayer(\n",
       "    (model): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (lin4): NetLinLayer(\n",
       "    (model): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiate Tensorflow Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the equivalent TensorFlow model\n",
    "tensorflow_model = LPIPSTF()"
   ],
   "metadata": {
    "id": "ZMybWnTDy2uE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before loading the weights, we first have to build the TensorFlow model with an input shape. Only then, we can proceed in loading int some weights."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test model to build it"
   ],
   "metadata": {
    "id": "ryWNzcxTmoIh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "i1 = np.random.rand(1, 3, 224, 224).astype(np.float32) * 255\n",
    "i2 = np.random.rand(1, 3, 224, 224).astype(np.float32) * 255\n",
    "\n",
    "pytorch_result = pytorch_model(\n",
    "    torch.from_numpy(i1).to(device),  # base PyTorch works with B,C,H,W format\n",
    "    torch.from_numpy(i2).to(device),\n",
    ")\n",
    "tensorflow_result = tensorflow_model([\n",
    "    np.moveaxis(i1, 1, -1),  # base TensorFlow works with B,H,W,C format.\n",
    "    np.moveaxis(i2, 1, -1),\n",
    "])"
   ],
   "metadata": {
    "id": "OTiT7jhhmnW-",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create the weights dictionary of the PyTorch model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "state_dict = pytorch_model.state_dict()\n",
    "{k: (v.dtype, v.shape) for k, v in pytorch_model.state_dict().items()}"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQ5Kr2Hdnu3g",
    "outputId": "36f40e6f-338d-43dc-9490-ed02a5ac02c9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'scaling_layer.shift': (torch.float32, torch.Size([1, 3, 1, 1])),\n",
       " 'scaling_layer.scale': (torch.float32, torch.Size([1, 3, 1, 1])),\n",
       " 'net.slice1.0.weight': (torch.float32, torch.Size([64, 3, 3, 3])),\n",
       " 'net.slice1.0.bias': (torch.float32, torch.Size([64])),\n",
       " 'net.slice1.2.weight': (torch.float32, torch.Size([64, 64, 3, 3])),\n",
       " 'net.slice1.2.bias': (torch.float32, torch.Size([64])),\n",
       " 'net.slice2.5.weight': (torch.float32, torch.Size([128, 64, 3, 3])),\n",
       " 'net.slice2.5.bias': (torch.float32, torch.Size([128])),\n",
       " 'net.slice2.7.weight': (torch.float32, torch.Size([128, 128, 3, 3])),\n",
       " 'net.slice2.7.bias': (torch.float32, torch.Size([128])),\n",
       " 'net.slice3.10.weight': (torch.float32, torch.Size([256, 128, 3, 3])),\n",
       " 'net.slice3.10.bias': (torch.float32, torch.Size([256])),\n",
       " 'net.slice3.12.weight': (torch.float32, torch.Size([256, 256, 3, 3])),\n",
       " 'net.slice3.12.bias': (torch.float32, torch.Size([256])),\n",
       " 'net.slice3.14.weight': (torch.float32, torch.Size([256, 256, 3, 3])),\n",
       " 'net.slice3.14.bias': (torch.float32, torch.Size([256])),\n",
       " 'net.slice4.17.weight': (torch.float32, torch.Size([512, 256, 3, 3])),\n",
       " 'net.slice4.17.bias': (torch.float32, torch.Size([512])),\n",
       " 'net.slice4.19.weight': (torch.float32, torch.Size([512, 512, 3, 3])),\n",
       " 'net.slice4.19.bias': (torch.float32, torch.Size([512])),\n",
       " 'net.slice4.21.weight': (torch.float32, torch.Size([512, 512, 3, 3])),\n",
       " 'net.slice4.21.bias': (torch.float32, torch.Size([512])),\n",
       " 'net.slice5.24.weight': (torch.float32, torch.Size([512, 512, 3, 3])),\n",
       " 'net.slice5.24.bias': (torch.float32, torch.Size([512])),\n",
       " 'net.slice5.26.weight': (torch.float32, torch.Size([512, 512, 3, 3])),\n",
       " 'net.slice5.26.bias': (torch.float32, torch.Size([512])),\n",
       " 'net.slice5.28.weight': (torch.float32, torch.Size([512, 512, 3, 3])),\n",
       " 'net.slice5.28.bias': (torch.float32, torch.Size([512])),\n",
       " 'lin0.model.1.weight': (torch.float32, torch.Size([1, 64, 1, 1])),\n",
       " 'lin1.model.1.weight': (torch.float32, torch.Size([1, 128, 1, 1])),\n",
       " 'lin2.model.1.weight': (torch.float32, torch.Size([1, 256, 1, 1])),\n",
       " 'lin3.model.1.weight': (torch.float32, torch.Size([1, 512, 1, 1])),\n",
       " 'lin4.model.1.weight': (torch.float32, torch.Size([1, 512, 1, 1]))}"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Move weights to TensorFlow model"
   ],
   "metadata": {
    "id": "Az0aoepxmxQO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scaling layer: Not really needed to transfer the weights, as we can manually define the 6 values used as mean/std to scale the input."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Scaling layer\n",
    "tensorflow_model.scaling_layer.set_weights([\n",
    "    np.reshape(state_dict['scaling_layer.shift'].cpu().numpy(), (1, 1, 1, 3)),\n",
    "    np.reshape(state_dict['scaling_layer.scale'].cpu().numpy(), (1, 1, 1, 3)),\n",
    "])"
   ],
   "metadata": {
    "id": "n7hqBeaonw7P"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "VGG16 layers: while the default pre-trained layers available in Keras can be used, I prefer use the same exact configuration used in the original implementation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VGG\n",
    "tensorflow_model.vgg.layers[0].layers[1].kernel.assign(  # Weights can be moved either by assigning a tf.Variable...\n",
    "    tf.Variable(\n",
    "        # Note: as explained before, PyTorch works as B,C,H,W: therefore, we need to transpose the matrix\n",
    "        # before assigning it!\n",
    "        state_dict['net.slice1.0.weight'].cpu().numpy().transpose(2, 3, 1, 0),\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    ")\n",
    "tensorflow_model.vgg.layers[0].layers[1].bias.assign(\n",
    "    tf.Variable(\n",
    "        state_dict['net.slice1.0.bias'].cpu().numpy(),\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    ")\n",
    "tensorflow_model.vgg.layers[0].layers[3].set_weights([  # Or by assigning a list of [kernel, bias] weights\n",
    "    np.transpose(state_dict['net.slice1.2.weight'].cpu().numpy(), (2, 3, 1, 0)),\n",
    "    state_dict['net.slice1.2.bias'].cpu().numpy(),\n",
    "])\n",
    "\n",
    "tensorflow_model.vgg.layers[1].layers[2].set_weights([\n",
    "    np.transpose(state_dict['net.slice2.5.weight'].cpu().numpy(), (2, 3, 1, 0)),\n",
    "    state_dict['net.slice2.5.bias'].cpu().numpy(),\n",
    "])\n",
    "tensorflow_model.vgg.layers[1].layers[4].set_weights([\n",
    "    np.transpose(state_dict['net.slice2.7.weight'].cpu().numpy(), (2, 3, 1, 0)),\n",
    "    state_dict['net.slice2.7.bias'].cpu().numpy(),\n",
    "])\n",
    "\n",
    "tensorflow_model.vgg.layers[2].layers[2].set_weights([\n",
    "    np.transpose(state_dict['net.slice3.10.weight'].cpu().numpy(), (2, 3, 1, 0)),\n",
    "    state_dict['net.slice3.10.bias'].cpu().numpy(),\n",
    "])\n",
    "tensorflow_model.vgg.layers[2].layers[4].set_weights([\n",
    "    np.transpose(state_dict['net.slice3.12.weight'].cpu().numpy(), (2, 3, 1, 0)),\n",
    "    state_dict['net.slice3.12.bias'].cpu().numpy(),\n",
    "])\n",
    "tensorflow_model.vgg.layers[2].layers[6].set_weights([\n",
    "    np.transpose(state_dict['net.slice3.14.weight'].cpu().numpy(), (2, 3, 1, 0)),\n",
    "    state_dict['net.slice3.14.bias'].cpu().numpy(),\n",
    "])\n",
    "\n",
    "tensorflow_model.vgg.layers[3].layers[2].set_weights([\n",
    "    np.transpose(state_dict['net.slice4.17.weight'].cpu().numpy(), (2, 3, 1, 0)),\n",
    "    state_dict['net.slice4.17.bias'].cpu().numpy(),\n",
    "])\n",
    "tensorflow_model.vgg.layers[3].layers[4].set_weights([\n",
    "    np.transpose(state_dict['net.slice4.19.weight'].cpu().numpy(), (2, 3, 1, 0)),\n",
    "    state_dict['net.slice4.19.bias'].cpu().numpy(),\n",
    "])\n",
    "tensorflow_model.vgg.layers[3].layers[6].set_weights([\n",
    "    np.transpose(state_dict['net.slice4.21.weight'].cpu().numpy(), (2, 3, 1, 0)),\n",
    "    state_dict['net.slice4.21.bias'].cpu().numpy(),\n",
    "])\n",
    "\n",
    "tensorflow_model.vgg.layers[4].layers[2].set_weights([\n",
    "    np.transpose(state_dict['net.slice5.24.weight'].cpu().numpy(), (2, 3, 1, 0)),\n",
    "    state_dict['net.slice5.24.bias'].cpu().numpy(),\n",
    "])\n",
    "tensorflow_model.vgg.layers[4].layers[4].set_weights([\n",
    "    np.transpose(state_dict['net.slice5.26.weight'].cpu().numpy(), (2, 3, 1, 0)),\n",
    "    state_dict['net.slice5.26.bias'].cpu().numpy(),\n",
    "])\n",
    "tensorflow_model.vgg.layers[4].layers[6].set_weights([\n",
    "    np.transpose(state_dict['net.slice5.28.weight'].cpu().numpy(), (2, 3, 1, 0)),\n",
    "    state_dict['net.slice5.28.bias'].cpu().numpy(),\n",
    "])"
   ],
   "metadata": {
    "id": "pse-1eD7rfT0"
   },
   "execution_count": 70,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Linear layers: the weights in this layers must all be positive! Otherwise, the final loss value could be negative. This happens because the squared differences passes through these layers, and then are averaged. Therefore, if the weights are negative, it can be possible for the output value loss to be negative.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(5):\n",
    "    tensorflow_model.linear_layers.layers[i].set_weights(\n",
    "        [np.transpose(state_dict[f'lin{i}.model.1.weight'].cpu().numpy(), (2, 3, 1, 0))]\n",
    "    )"
   ],
   "metadata": {
    "id": "dVHpl4rFtjyQ"
   },
   "execution_count": 71,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(5):\n",
    "    print(np.min(state_dict[f'lin{i}.model.1.weight'].cpu().numpy()))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IU1yBIs4EvxS",
    "outputId": "695b84dd-afd0-4fed-9391-f734dbbdecae"
   },
   "execution_count": 72,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.021017654\n",
      "0.009468972\n",
      "0.037710946\n",
      "0.039098237\n",
      "0.019278834\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare the results between the two models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "i1 = np.random.rand(1, 3, 224, 224).astype(np.float32)\n",
    "i2 = np.random.rand(1, 3, 224, 224).astype(np.float32)\n",
    "\n",
    "pytorch_result = pytorch_model(\n",
    "    torch.from_numpy(i1).to(device),\n",
    "    torch.from_numpy(i2).to(device),\n",
    ")\n",
    "tensorflow_result = tensorflow_model([\n",
    "    np.moveaxis(i1, 1, -1),\n",
    "    np.moveaxis(i2, 1, -1),\n",
    "])"
   ],
   "metadata": {
    "id": "DKE0xl-ma-2J"
   },
   "execution_count": 73,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.allclose(pytorch_result[0].cpu().numpy().squeeze(), tensorflow_result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEu8_X-mbG-n",
    "outputId": "7cada2ae-e28b-4706-ea07-6925b18db5b8"
   },
   "execution_count": 74,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    i1 = np.random.rand(1, 3, 224, 224).astype(np.float32) * 255\n",
    "    i2 = np.random.rand(1, 3, 224, 224).astype(np.float32) * 255\n",
    "\n",
    "    pytorch_result = pytorch_model(\n",
    "        torch.from_numpy(i1).to(device),\n",
    "        torch.from_numpy(i2).to(device),\n",
    "    )\n",
    "    tensorflow_result = tensorflow_model([\n",
    "        np.moveaxis(i1, 1, -1),\n",
    "        np.moveaxis(i2, 1, -1)]).numpy()\n",
    "\n",
    "    print(\"pytorch=\", pytorch_result)\n",
    "    print(\"tensorflow=\", tensorflow_result)\n",
    "    np.allclose(pytorch_result.cpu().numpy().squeeze(), tensorflow_result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaM58ZQ9_zaU",
    "outputId": "c5cd12fc-29e2-425f-e6c6-2d47ee2e0e37"
   },
   "execution_count": 75,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pytorch= tensor([[[[0.2380]]]], device='cuda:0')\n",
      "tensorflow= 0.23802666\n",
      "pytorch= tensor([[[[0.2398]]]], device='cuda:0')\n",
      "tensorflow= 0.23982744\n",
      "pytorch= tensor([[[[0.2345]]]], device='cuda:0')\n",
      "tensorflow= 0.23448285\n",
      "pytorch= tensor([[[[0.2383]]]], device='cuda:0')\n",
      "tensorflow= 0.23829569\n",
      "pytorch= tensor([[[[0.2317]]]], device='cuda:0')\n",
      "tensorflow= 0.23170039\n",
      "pytorch= tensor([[[[0.2382]]]], device='cuda:0')\n",
      "tensorflow= 0.23820105\n",
      "pytorch= tensor([[[[0.2323]]]], device='cuda:0')\n",
      "tensorflow= 0.23226494\n",
      "pytorch= tensor([[[[0.2399]]]], device='cuda:0')\n",
      "tensorflow= 0.2399226\n",
      "pytorch= tensor([[[[0.2374]]]], device='cuda:0')\n",
      "tensorflow= 0.23743632\n",
      "pytorch= tensor([[[[0.2359]]]], device='cuda:0')\n",
      "tensorflow= 0.23594017\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    i1 = np.random.rand(1, 3, 224, 224).astype(np.float32) * 255\n",
    "    i2 = np.random.rand(1, 3, 224, 224).astype(np.float32) * 255\n",
    "\n",
    "    pytorch_result = pytorch_model(\n",
    "        torch.from_numpy(i1).to(device),\n",
    "        torch.from_numpy(i1 * i2).to(device),\n",
    "    )\n",
    "    tensorflow_result = tensorflow_model([\n",
    "        np.moveaxis(i1, 1, -1),\n",
    "        np.moveaxis(i1 * i2, 1, -1)]).numpy()\n",
    "\n",
    "    print(pytorch_result)\n",
    "    print(tensorflow_result)\n",
    "    assert np.allclose(pytorch_result.cpu().numpy().squeeze(), tensorflow_result, )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcvzyM56FiN5",
    "outputId": "dff4ac31-04bc-4ebb-f207-18542eb4a6fc"
   },
   "execution_count": 76,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[0.1988]]]], device='cuda:0')\n",
      "0.19875127\n",
      "tensor([[[[0.2047]]]], device='cuda:0')\n",
      "0.20469661\n",
      "tensor([[[[0.1980]]]], device='cuda:0')\n",
      "0.19804001\n",
      "tensor([[[[0.2132]]]], device='cuda:0')\n",
      "0.21316913\n",
      "tensor([[[[0.2019]]]], device='cuda:0')\n",
      "0.20190895\n",
      "tensor([[[[0.1955]]]], device='cuda:0')\n",
      "0.1955429\n",
      "tensor([[[[0.2011]]]], device='cuda:0')\n",
      "0.20108235\n",
      "tensor([[[[0.1892]]]], device='cuda:0')\n",
      "0.18921155\n",
      "tensor([[[[0.1958]]]], device='cuda:0')\n",
      "0.1958212\n",
      "tensor([[[[0.1880]]]], device='cuda:0')\n",
      "0.18803968\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tensorflow_model.save(\"./lpips\")\n",
    "tensorflow_model.save_weights(\"weights.h5\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MP1sO_J_jd8N",
    "outputId": "1071262d-85a0-484a-df77-66b2ae200413"
   },
   "execution_count": 77,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check that there are no problems with the loaded model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "new_model = tf.keras.models.load_model(\"./lpips\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PeVJYz7ckAL_",
    "outputId": "92568e2a-4cf5-4ad4-d15b-096aa2212abb"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    i1 = np.random.rand(1, 3, 224, 224).astype(np.float32) * 255\n",
    "    i2 = np.random.rand(1, 3, 224, 224).astype(np.float32) * 255\n",
    "\n",
    "    pytorch_result = pytorch_model(\n",
    "        torch.from_numpy(i1).to(device),\n",
    "        torch.from_numpy(i1 * i2).to(device),\n",
    "    )\n",
    "    tensorflow_result = new_model([\n",
    "        np.moveaxis(i1, 1, -1),\n",
    "        np.moveaxis(i1 * i2, 1, -1)]).numpy()\n",
    "\n",
    "    print(pytorch_result)\n",
    "    print(tensorflow_result)\n",
    "    assert np.allclose(pytorch_result.cpu().numpy().squeeze(), tensorflow_result, )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4nI6BaVHkxFH",
    "outputId": "9014500a-c7f1-4f87-c72a-7223486c7042"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[0.0071]]]], device='cuda:0')\n",
      "0.0070816744\n",
      "tensor([[[[0.0079]]]], device='cuda:0')\n",
      "0.007931068\n",
      "tensor([[[[0.0087]]]], device='cuda:0')\n",
      "0.00866513\n",
      "tensor([[[[0.0083]]]], device='cuda:0')\n",
      "0.008283554\n",
      "tensor([[[[0.0084]]]], device='cuda:0')\n",
      "0.008445184\n",
      "tensor([[[[0.0069]]]], device='cuda:0')\n",
      "0.006898164\n",
      "tensor([[[[0.0064]]]], device='cuda:0')\n",
      "0.006368886\n",
      "tensor([[[[0.0060]]]], device='cuda:0')\n",
      "0.0060019363\n",
      "tensor([[[[0.0073]]]], device='cuda:0')\n",
      "0.007324224\n",
      "tensor([[[[0.0078]]]], device='cuda:0')\n",
      "0.007751484\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!zip -r archive.zip./ lpips"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OouQS5Cuk2Hp",
    "outputId": "4e8bebd7-76c2-494a-859a-4e8c563ab450"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  adding: lpips/ (stored 0%)\n",
      "  adding: lpips/variables/ (stored 0%)\n",
      "  adding: lpips/variables/variables.data-00000-of-00001 (deflated 7%)\n",
      "  adding: lpips/variables/variables.index (deflated 66%)\n",
      "  adding: lpips/assets/ (stored 0%)\n",
      "  adding: lpips/fingerprint.pb (stored 0%)\n",
      "  adding: lpips/saved_model.pb (deflated 91%)\n",
      "  adding: lpips/keras_metadata.pb (deflated 95%)\n"
     ]
    }
   ]
  }
 ]
}
